{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.34827760\n",
      "Iteration 2, loss = 2.10472333\n",
      "Iteration 3, loss = 1.89391017\n",
      "Iteration 4, loss = 1.69788802\n",
      "Iteration 5, loss = 1.50570708\n",
      "Iteration 6, loss = 1.33446597\n",
      "Iteration 7, loss = 1.18280574\n",
      "Iteration 8, loss = 1.05181027\n",
      "Iteration 9, loss = 0.93778717\n",
      "Iteration 10, loss = 0.83944038\n",
      "Iteration 11, loss = 0.75488616\n",
      "Iteration 12, loss = 0.68293745\n",
      "Iteration 13, loss = 0.62186096\n",
      "Iteration 14, loss = 0.56849259\n",
      "Iteration 15, loss = 0.52363768\n",
      "Iteration 16, loss = 0.48430194\n",
      "Iteration 17, loss = 0.45082990\n",
      "Iteration 18, loss = 0.42075892\n",
      "Iteration 19, loss = 0.39374293\n",
      "Iteration 20, loss = 0.37048768\n",
      "Iteration 21, loss = 0.34985510\n",
      "Iteration 22, loss = 0.33093358\n",
      "Iteration 23, loss = 0.31463720\n",
      "Iteration 24, loss = 0.29966601\n",
      "Iteration 25, loss = 0.28568251\n",
      "Iteration 26, loss = 0.27315025\n",
      "Iteration 27, loss = 0.26217843\n",
      "Iteration 28, loss = 0.25126747\n",
      "Iteration 29, loss = 0.24162457\n",
      "Iteration 30, loss = 0.23315972\n",
      "Iteration 31, loss = 0.22475351\n",
      "Iteration 32, loss = 0.21735012\n",
      "Iteration 33, loss = 0.20995515\n",
      "Iteration 34, loss = 0.20322930\n",
      "Iteration 35, loss = 0.19719748\n",
      "Iteration 36, loss = 0.19180762\n",
      "Iteration 37, loss = 0.18605191\n",
      "Iteration 38, loss = 0.18073174\n",
      "Iteration 39, loss = 0.17626382\n",
      "Iteration 40, loss = 0.17129526\n",
      "Iteration 41, loss = 0.16742472\n",
      "Iteration 42, loss = 0.16274321\n",
      "Iteration 43, loss = 0.15882879\n",
      "Iteration 44, loss = 0.15495992\n",
      "Iteration 45, loss = 0.15170731\n",
      "Iteration 46, loss = 0.14821314\n",
      "Iteration 47, loss = 0.14508703\n",
      "Iteration 48, loss = 0.14196788\n",
      "Iteration 49, loss = 0.13882684\n",
      "Iteration 50, loss = 0.13597193\n",
      "Iteration 51, loss = 0.13314984\n",
      "Iteration 52, loss = 0.13060134\n",
      "Iteration 53, loss = 0.12782583\n",
      "Iteration 54, loss = 0.12556969\n",
      "Iteration 55, loss = 0.12314882\n",
      "Iteration 56, loss = 0.12103636\n",
      "Iteration 57, loss = 0.11870951\n",
      "Iteration 58, loss = 0.11671972\n",
      "Iteration 59, loss = 0.11466924\n",
      "Iteration 60, loss = 0.11264687\n",
      "Iteration 61, loss = 0.11075232\n",
      "Iteration 62, loss = 0.10898803\n",
      "Iteration 63, loss = 0.10729701\n",
      "Iteration 64, loss = 0.10553145\n",
      "Iteration 65, loss = 0.10379117\n",
      "Iteration 66, loss = 0.10212177\n",
      "Iteration 67, loss = 0.10075114\n",
      "Iteration 68, loss = 0.09910836\n",
      "Iteration 69, loss = 0.09760955\n",
      "Iteration 70, loss = 0.09647862\n",
      "Iteration 71, loss = 0.09512264\n",
      "Iteration 72, loss = 0.09354872\n",
      "Iteration 73, loss = 0.09241402\n",
      "Iteration 74, loss = 0.09092405\n",
      "Iteration 75, loss = 0.08978849\n",
      "Iteration 76, loss = 0.08859228\n",
      "Iteration 77, loss = 0.08755666\n",
      "Iteration 78, loss = 0.08632728\n",
      "Iteration 79, loss = 0.08521534\n",
      "Iteration 80, loss = 0.08410886\n",
      "Iteration 81, loss = 0.08299535\n",
      "Iteration 82, loss = 0.08187878\n",
      "Iteration 83, loss = 0.08094405\n",
      "Iteration 84, loss = 0.07989811\n",
      "Iteration 85, loss = 0.07890596\n",
      "Iteration 86, loss = 0.07807342\n",
      "Iteration 87, loss = 0.07715264\n",
      "Iteration 88, loss = 0.07632109\n",
      "Iteration 89, loss = 0.07535259\n",
      "Iteration 90, loss = 0.07483183\n",
      "Iteration 91, loss = 0.07363674\n",
      "Iteration 92, loss = 0.07285583\n",
      "Iteration 93, loss = 0.07208376\n",
      "Iteration 94, loss = 0.07119209\n",
      "Iteration 95, loss = 0.07050566\n",
      "Iteration 96, loss = 0.06981500\n",
      "Iteration 97, loss = 0.06920199\n",
      "Iteration 98, loss = 0.06817634\n",
      "Iteration 99, loss = 0.06773667\n",
      "Iteration 100, loss = 0.06675130\n",
      "Iteration 101, loss = 0.06622905\n",
      "Iteration 102, loss = 0.06556957\n",
      "Iteration 103, loss = 0.06493093\n",
      "Iteration 104, loss = 0.06444187\n",
      "Iteration 105, loss = 0.06356779\n",
      "Iteration 106, loss = 0.06300418\n",
      "Iteration 107, loss = 0.06239095\n",
      "Iteration 108, loss = 0.06179656\n",
      "Iteration 109, loss = 0.06114846\n",
      "Iteration 110, loss = 0.06060613\n",
      "Iteration 111, loss = 0.06016893\n",
      "Iteration 112, loss = 0.05943207\n",
      "Iteration 113, loss = 0.05900849\n",
      "Iteration 114, loss = 0.05845453\n",
      "Iteration 115, loss = 0.05810620\n",
      "Iteration 116, loss = 0.05737858\n",
      "Iteration 117, loss = 0.05688368\n",
      "Iteration 118, loss = 0.05642631\n",
      "Iteration 119, loss = 0.05588008\n",
      "Iteration 120, loss = 0.05540849\n",
      "Iteration 121, loss = 0.05496006\n",
      "Iteration 122, loss = 0.05449525\n",
      "Iteration 123, loss = 0.05396433\n",
      "Iteration 124, loss = 0.05356122\n",
      "Iteration 125, loss = 0.05318137\n",
      "Iteration 126, loss = 0.05256936\n",
      "Iteration 127, loss = 0.05213052\n",
      "Iteration 128, loss = 0.05182296\n",
      "Iteration 129, loss = 0.05134240\n",
      "Iteration 130, loss = 0.05096873\n",
      "Iteration 131, loss = 0.05049419\n",
      "Iteration 132, loss = 0.05038462\n",
      "Iteration 133, loss = 0.04976794\n",
      "Iteration 134, loss = 0.04937136\n",
      "Iteration 135, loss = 0.04895093\n",
      "Iteration 136, loss = 0.04853682\n",
      "Iteration 137, loss = 0.04830534\n",
      "Iteration 138, loss = 0.04783068\n",
      "Iteration 139, loss = 0.04750819\n",
      "Iteration 140, loss = 0.04707833\n",
      "Iteration 141, loss = 0.04673185\n",
      "Iteration 142, loss = 0.04635781\n",
      "Iteration 143, loss = 0.04601611\n",
      "Iteration 144, loss = 0.04573570\n",
      "Iteration 145, loss = 0.04549470\n",
      "Iteration 146, loss = 0.04508107\n",
      "Iteration 147, loss = 0.04475669\n",
      "Iteration 148, loss = 0.04444467\n",
      "Iteration 149, loss = 0.04409286\n",
      "Iteration 150, loss = 0.04382674\n",
      "Iteration 151, loss = 0.04354958\n",
      "Iteration 152, loss = 0.04316949\n",
      "Iteration 153, loss = 0.04299275\n",
      "Iteration 154, loss = 0.04266982\n",
      "Iteration 155, loss = 0.04230386\n",
      "Iteration 156, loss = 0.04205188\n",
      "Iteration 157, loss = 0.04182215\n",
      "Iteration 158, loss = 0.04148990\n",
      "Iteration 159, loss = 0.04120774\n",
      "Iteration 160, loss = 0.04091385\n",
      "Iteration 161, loss = 0.04069099\n",
      "Iteration 162, loss = 0.04046331\n",
      "Iteration 163, loss = 0.04014115\n",
      "Iteration 164, loss = 0.03982438\n",
      "Iteration 165, loss = 0.03961729\n",
      "Iteration 166, loss = 0.03936227\n",
      "Iteration 167, loss = 0.03910441\n",
      "Iteration 168, loss = 0.03879208\n",
      "Iteration 169, loss = 0.03853471\n",
      "Iteration 170, loss = 0.03843007\n",
      "Iteration 171, loss = 0.03803722\n",
      "Iteration 172, loss = 0.03781855\n",
      "Iteration 173, loss = 0.03749524\n",
      "Iteration 174, loss = 0.03728487\n",
      "Iteration 175, loss = 0.03704063\n",
      "Iteration 176, loss = 0.03685115\n",
      "Iteration 177, loss = 0.03654672\n",
      "Iteration 178, loss = 0.03640978\n",
      "Iteration 179, loss = 0.03614404\n",
      "Iteration 180, loss = 0.03600696\n",
      "Iteration 181, loss = 0.03575634\n",
      "Iteration 182, loss = 0.03553480\n",
      "Iteration 183, loss = 0.03531863\n",
      "Iteration 184, loss = 0.03507267\n",
      "Iteration 185, loss = 0.03489329\n",
      "Iteration 186, loss = 0.03467284\n",
      "Iteration 187, loss = 0.03446877\n",
      "Iteration 188, loss = 0.03430637\n",
      "Iteration 189, loss = 0.03418394\n",
      "Iteration 190, loss = 0.03397435\n",
      "Iteration 191, loss = 0.03368212\n",
      "Iteration 192, loss = 0.03354273\n",
      "Iteration 193, loss = 0.03334655\n",
      "Iteration 194, loss = 0.03315828\n",
      "Iteration 195, loss = 0.03294722\n",
      "Iteration 196, loss = 0.03283490\n",
      "Iteration 197, loss = 0.03262684\n",
      "Iteration 198, loss = 0.03248364\n",
      "Iteration 199, loss = 0.03234182\n",
      "Iteration 200, loss = 0.03201755\n",
      "[4 4 7 2 8 2 2 5 7 9 5 4 8 1 4 9 0 8 9 8]\n",
      "[4 4 7 2 8 2 2 5 7 9 5 4 8 8 4 9 0 8 9 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabre\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), activation=\"logistic\", max_iter=200,alpha = 0.001,solver=\"sgd\", tol=1e-9,learning_rate_init=.01,verbose=True)\n",
    "\n",
    "mlp = mlp.fit(digits.data[:-20], digits.target[:-20])\n",
    "\n",
    "print(mlp.predict(digits.data[-20:]))\n",
    "print(digits.target[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
